{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedbf084",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad498013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주피터\n",
    "import os\n",
    "\n",
    "# os.chdir('G:/내 드라이브/projects/NLP-StockMarket/model_fin/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78893bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코랩\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "import os\n",
    "path = '/content/drive/My Drive/projects/NLP-StockMarket/model_fin/' # 본인 구글 드라이브 계정마다 살짝씩 다를 수도 있음\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff80718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "814b61de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:08:21.118313Z",
     "start_time": "2022-07-13T03:08:21.073913Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, ReadInstruction\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, TFBertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d947f09",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58625705",
   "metadata": {},
   "source": [
    "### 종목명 선택 및 뉴스,토론방, 유튜브 데이터 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e07c0d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:00:23.091353Z",
     "start_time": "2022-07-13T02:00:23.078099Z"
    }
   },
   "outputs": [],
   "source": [
    "# LG화학, 삼성SDI, SK이노베이션, 고려아연, 포스코케미칼\n",
    "stock_name = '삼성SDI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "818dcc34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:02.465765Z",
     "start_time": "2022-07-13T02:09:01.051158Z"
    }
   },
   "outputs": [],
   "source": [
    "naver_news = pd.read_csv('./data/refined_naver_news.csv', index_col = 0)\n",
    "daum_news = pd.read_csv('./data/refined_daum_news.csv', index_col = 0)\n",
    "naver_talks = pd.read_csv(f'./data/refined_naver_talks_{stock_name}.csv', index_col = 0)\n",
    "daum_talks = pd.read_csv(f'./data/refined_daum_talks_{stock_name}.csv', index_col = 0)\n",
    "youtube = pd.read_csv(f'./data/refined_youtube_{stock_name}.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2768f2df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:02.963703Z",
     "start_time": "2022-07-13T02:09:02.697015Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 통합\n",
    "news_df = pd.concat([naver_news, daum_news, naver_talks, daum_talks ,youtube])\n",
    "\n",
    "# 'Date' 타입이 int 이므로 datetime으로 변환\n",
    "news_df['Date'] = pd.to_datetime(news_df['Date'].astype(str))\n",
    "\n",
    "# 합쳐진 데이터들의 인덱스 재설정\n",
    "news_df.sort_values('Date', ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6b2fe2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:03.025661Z",
     "start_time": "2022-07-13T02:09:02.996571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>비올  일본 최대 병원체인과 실펌엑스 총판계약 체결</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>환율 하락 전환  1086 2 감소0 1원</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date                         Title\n",
       "639 2021-01-04  비올  일본 최대 병원체인과 실펌엑스 총판계약 체결\n",
       "640 2021-01-04      환율 하락 전환  1086 2 감소0 1원 "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2021년 주식 데이터가 1월 4일부터 있어서 슬라이싱\n",
    "news_df = news_df[news_df[news_df['Date']== '2021-01-04'].index[0] : ]\n",
    "news_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a63c5",
   "metadata": {},
   "source": [
    "### 주가 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19959ffe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:03.847089Z",
     "start_time": "2022-07-13T02:09:03.829114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>일자</th>\n",
       "      <th>등락률</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          일자   등락률\n",
       "0 2021-01-04  6.85\n",
       "1 2021-01-05  2.24"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df = pd.read_csv(f'./data/{stock_name}_주가_데이터.csv', usecols = ['일자','등락률'])\n",
    "stock_df['일자'] = pd.to_datetime(stock_df['일자'])\n",
    "stock_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e303205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:04.032080Z",
     "start_time": "2022-07-13T02:09:04.016957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-04 00:00:00\n",
      "2022-06-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "start = str(stock_df.iloc[0, 0])\n",
    "end = str(stock_df.iloc[-1, 0])\n",
    "print(start)\n",
    "print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dde2f58d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:04.203593Z",
     "start_time": "2022-07-13T02:09:04.195466Z"
    }
   },
   "outputs": [],
   "source": [
    "stock_df['updown'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75939d92",
   "metadata": {},
   "source": [
    "## 데이터 프레임 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8b85f65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:05.543455Z",
     "start_time": "2022-07-13T02:09:05.529220Z"
    }
   },
   "outputs": [],
   "source": [
    "## 뉴스일자 조정(예측대상(주가)의 일자와 맞추기 위해)\n",
    "news_df['일자'] = news_df['Date'] + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c3c92e33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:28.110050Z",
     "start_time": "2022-07-13T02:09:27.772006Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Title</th>\n",
       "      <th>주가의 날짜</th>\n",
       "      <th>등락률</th>\n",
       "      <th>updown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>비올  일본 최대 병원체인과 실펌엑스 총판계약 체결</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>환율 하락 전환  1086 2 감소0 1원</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>코스피  1 03p 0 04   오른 2874 50 출발  원 달러 환율 1 2원 ...</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>韓증시 사상최고치 시작  개장식선 안정적 시장운영에 방점</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>SK바이오팜  아벨 지분 매각으로 5천500만 달러 자본이득</td>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                                              Title     주가의 날짜  \\\n",
       "0 2021-01-04                       비올  일본 최대 병원체인과 실펌엑스 총판계약 체결 2021-01-05   \n",
       "1 2021-01-04                           환율 하락 전환  1086 2 감소0 1원  2021-01-05   \n",
       "2 2021-01-04  코스피  1 03p 0 04   오른 2874 50 출발  원 달러 환율 1 2원 ... 2021-01-05   \n",
       "3 2021-01-04                    韓증시 사상최고치 시작  개장식선 안정적 시장운영에 방점 2021-01-05   \n",
       "4 2021-01-04                  SK바이오팜  아벨 지분 매각으로 5천500만 달러 자본이득 2021-01-05   \n",
       "\n",
       "    등락률  updown  \n",
       "0  2.24       0  \n",
       "1  2.24       0  \n",
       "2  2.24       0  \n",
       "3  2.24       0  \n",
       "4  2.24       0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = news_df.merge(stock_df)\n",
    "df.columns = [df.columns[0], df.columns[1], '주가의 날짜', '등락률', 'updown']\n",
    "df.drop_duplicates('Title', inplace = True, ignore_index = True)  # 기사제목 중복 제거\n",
    "print(len(df))\n",
    "df.dropna(inplace =True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "65c2c220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:30.283236Z",
     "start_time": "2022-07-13T02:09:30.262289Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.iloc[: 4500, :]             ## 임시!!!\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1dda3a",
   "metadata": {},
   "source": [
    "## 감성사전 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "092df12f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:32.630023Z",
     "start_time": "2022-07-13T02:09:32.611046Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>mid</th>\n",
       "      <th>neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>방긋</td>\n",
       "      <td>아직</td>\n",
       "      <td>회의적</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>상회</td>\n",
       "      <td>보통</td>\n",
       "      <td>바닥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>신선</td>\n",
       "      <td>vs</td>\n",
       "      <td>이탈</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>신박</td>\n",
       "      <td>중립</td>\n",
       "      <td>떨어지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>투혼</td>\n",
       "      <td>관망</td>\n",
       "      <td>안좋게</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos mid  neg\n",
       "0  방긋  아직  회의적\n",
       "1  상회  보통   바닥\n",
       "2  신선  vs   이탈\n",
       "3  신박  중립  떨어지\n",
       "4  투혼  관망  안좋게"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_csv = pd.read_csv('./sentiment dictionary.csv', index_col = 0)\n",
    "sentiment_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1d2ea5af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:32.953631Z",
     "start_time": "2022-07-13T02:09:32.938643Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_li = sentiment_csv['pos'].dropna().values\n",
    "mid_li = sentiment_csv['mid'].dropna().values\n",
    "neg_li = sentiment_csv['neg'].dropna().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09998cbe",
   "metadata": {},
   "source": [
    "### 감성 지수 계산하는 sentimental_score() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07d4d044",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:33.624167Z",
     "start_time": "2022-07-13T02:09:33.610978Z"
    }
   },
   "outputs": [],
   "source": [
    "### binary_sentimental_score\n",
    "def binary_sentimental_score(df):\n",
    "    # 입력받은 데이터프레임 복사 및 컬럼 추가\n",
    "    df_result = df.copy()\n",
    "    df_result['Pos'] = 0\n",
    "    df_result['Neg'] = 0\n",
    "    df_result['감성지수'] = 0\n",
    "    \n",
    "    # SP 등락률에 따른 updown 계산\n",
    "    df_result.loc[df_result.query('등락률 >= 0').index, 'updown'] = 1\n",
    "    df_result.loc[df_result.query('등락률 < 0').index, 'updown'] = -1\n",
    "\n",
    "    # 감성 지수는 긍정 : 1, 부정 : -1, 해당 데이터 제외 : 999\n",
    "    df_result['감성지수'] = 999    \n",
    "    \n",
    "    # 감성 사전에 따른 텍스트 검출\n",
    "    print('긍정 단어 검색중')\n",
    "    for pos in tqdm(pos_li) :\n",
    "        str_expr = f\"Title.str.contains('{pos}')\"\n",
    "        df_result.loc[df_result.query(str_expr).index, 'Pos'] = 1\n",
    "    \n",
    "    print('부정 단어 검색중')\n",
    "    for neg in tqdm(neg_li) :\n",
    "        str_expr = f\"Title.str.contains('{neg}')\"\n",
    "        df_result.loc[df_result.query(str_expr).index, 'Neg'] = 1\n",
    "    \n",
    "    # 긍정 단어만이 검출되면 긍정\n",
    "    df_result.loc[df_result.query('Pos == 1 and Neg == 0').index, '감성지수'] = 1\n",
    "    \n",
    "    # 부정 단어만이 검출되면 부정\n",
    "    df_result.loc[df_result.query('Pos == 0 and Neg == 1').index, '감성지수'] = -1\n",
    "    \n",
    "    # 긍정, 부정 단어가 둘 다 있으면 전 날 또는 당일 주가의 등락률을 보고 결정\n",
    "    print('긍정 부정 둘 다 있는 경우 처리중')\n",
    "    for i in tqdm(df_result.loc[df_result.query('Pos == 1 and Neg == 1').index].index) : \n",
    "        updown = 999 # 등락률을 뜻하는 updown\n",
    "        \n",
    "        # 해당 Title의 어제 주가가 있으면 선택\n",
    "        if sum(df_result.loc[i,'Date'] - timedelta(days = 1) == stock_df['일자']) == 1 :  \n",
    "            updown = stock_df[stock_df['일자'] == df_result.loc[i,'Date'] - timedelta(days = 1)]['등락률'].values[0]\n",
    "        \n",
    "        # 어제 주가는 없지만 당일이 있으면 당일을 선택\n",
    "        elif sum(df_result.loc[i,'Date'] == stock_df['일자']) == 1 :  \n",
    "            updown = stock_df[stock_df['일자'] == df_result.loc[i,'Date']]['등락률'].values[0]\n",
    "        # 어제와 오늘의 주가도 없다면 이전의 주가를 찾아 탐색\n",
    "        else :\n",
    "            j = 2 \n",
    "            while True :\n",
    "                if sum(df_result.loc[i,'Date'] - timedelta(days = j) == stock_df['일자']) == 1 :\n",
    "                    updown = stock_df[stock_df['일자'] == df_result.loc[i,'Date'] - timedelta(days = j)]['등락률'].values[0]\n",
    "                    break\n",
    "                j += 1\n",
    "        \n",
    "        # 절댓값이 0보다 낮은 등락률은 변화가 없다고 판단\n",
    "        if updown > 0 :\n",
    "            df_result.loc[i,'감성지수'] = 1\n",
    "        else :\n",
    "            df_result.loc[i,'감성지수'] = -1\n",
    "    df_result = pd.concat([df_result.loc[df_result.query('감성지수 != 999').index]], ignore_index = True)\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3d716df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:34.103238Z",
     "start_time": "2022-07-13T02:09:34.076628Z"
    }
   },
   "outputs": [],
   "source": [
    "### multi_sentimental_score\n",
    "def multi_sentimental_score(df):\n",
    "    # 입력받은 데이터프레임 복사 및 컬럼 추가\n",
    "    df_result = df.copy()\n",
    "    df_result['Pos'] = 0\n",
    "    df_result['Neg'] = 0\n",
    "    df_result['Mid'] = 0\n",
    "    df_result['감성지수'] = 0\n",
    "    \n",
    "    # SP 등락률에 따른 updown 계산\n",
    "    df_result.loc[df_result.query('등락률 > 1').index, 'updown'] = 1\n",
    "    df_result.loc[df_result.query('등락률 < -1').index, 'updown'] = -1\n",
    "    \n",
    "    # 감성 지수는 긍정 : 1, 중립 : 0, 부정 : -1, 해당 데이터 제외 : 999\n",
    "    df_result['감성지수'] = 999    \n",
    "    \n",
    "################################################################################################\n",
    "    # 0  : 없음, 1: 있음\n",
    "    # 감성 사전에 따른 텍스트 검출\n",
    "    print('긍정 단어 검색중')\n",
    "    for pos in tqdm(pos_li) :\n",
    "        str_expr = f\"Title.str.contains('{pos}')\"\n",
    "        df_result.loc[df_result.query(str_expr).index, 'Pos'] = 1\n",
    "    \n",
    "    print('부정 단어 검색중')\n",
    "    for neg in tqdm(neg_li) :\n",
    "        str_expr = f\"Title.str.contains('{neg}')\"\n",
    "        df_result.loc[df_result.query(str_expr).index, 'Neg'] = 1\n",
    "    \n",
    "    print('중립 단어 검색중')\n",
    "    for mid in tqdm(mid_li) :\n",
    "        str_expr = f\"Title.str.contains('{mid}')\"\n",
    "        df_result.loc[df_result.query(str_expr).index, 'Mid'] = 1\n",
    "    \n",
    "    \n",
    "################################################################################################\n",
    "    \n",
    "    # 모든 종류의 단어가 검출 되면 제외\n",
    "    df_result.loc[df_result.query('Pos == 1 and Neg == 1 and Mid == 1').index, '감성지수'] = 999\n",
    "    \n",
    "    # 중립 단어가 검출되면 중립\n",
    "    df_result.loc[df_result.query('Mid == 1').index, '감성지수'] = 0\n",
    "    \n",
    "    # 긍정 단어만이 검출되면 긍정\n",
    "    df_result.loc[df_result.query('Pos == 1 and Neg == 0 and Mid == 0').index, '감성지수'] = 1\n",
    "    \n",
    "    # 부정 단어만이 검출되면 부정\n",
    "    df_result.loc[df_result.query('Pos == 0 and Neg == 1 and Mid == 0').index, '감성지수'] = -1\n",
    "    \n",
    "    \n",
    "    # 긍정, 부정 단어가 둘 다 있으면 전 날 또는 당일 주가의 등락률을 보고 결정\n",
    "    print('긍정 부정 둘 다 있는 경우 처리중')\n",
    "    for i in tqdm(df_result.loc[df_result.query('Pos == 1 and Neg == 1 and Mid == 0').index].index) : \n",
    "        \n",
    "        updown = 999 # 등락률을 뜻하는 updown\n",
    "        \n",
    "        # 해당 Title의 어제 주가가 있으면 선택\n",
    "        if sum(df_result.loc[i,'Date'] - timedelta(days = 1) == stock_df['일자']) == 1 :  \n",
    "            updown = stock_df[stock_df['일자'] == df_result.loc[i,'Date'] - timedelta(days = 1)]['등락률'].values[0]\n",
    "        \n",
    "        # 어제 주가는 없지만 당일이 있으면 당일을 선택\n",
    "        elif sum(df_result.loc[i,'Date'] == stock_df['일자']) == 1 :  \n",
    "            updown = stock_df[stock_df['일자'] == df_result.loc[i,'Date']]['등락률'].values[0]\n",
    "        # 어제와 오늘의 주가도 없다면 이전의 주가를 찾아 탐색\n",
    "        else :\n",
    "            j = 2 \n",
    "            while True :\n",
    "                if sum(df_result.loc[i,'Date'] - timedelta(days = j) == stock_df['일자']) == 1 :\n",
    "                    updown = stock_df[stock_df['일자'] == df_result.loc[i,'Date'] - timedelta(days = j)]['등락률'].values[0]\n",
    "                    break\n",
    "                j += 1\n",
    "        \n",
    "        # 절댓값이 0보다 낮은 등락률은 변화가 없다고 판단\n",
    "        if updown > 1 :\n",
    "            df_result.loc[i,'감성지수'] = 1\n",
    "        elif updown < -1 :\n",
    "            df_result.loc[i,'감성지수'] = -1\n",
    "        else :\n",
    "            df_result.loc[i,'감성지수'] = 0\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e285260c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:09:41.440763Z",
     "start_time": "2022-07-13T02:09:34.763034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 단어 검색중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 648/648 [00:02<00:00, 278.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정 단어 검색중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1001/1001 [00:03<00:00, 281.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중립 단어 검색중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 53/53 [00:00<00:00, 259.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 부정 둘 다 있는 경우 처리중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 782/782 [00:00<00:00, 1414.90it/s]\n"
     ]
    }
   ],
   "source": [
    "df_result = multi_sentimental_score(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdead8",
   "metadata": {},
   "source": [
    "## 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58e482b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:18.297682Z",
     "start_time": "2022-07-13T02:13:18.281979Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df_result, shuffle = False, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9f7cfa79",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:18.857775Z",
     "start_time": "2022-07-13T02:13:18.849986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4050, 9)\n",
      "(450, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7e114e08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:19.434393Z",
     "start_time": "2022-07-13T02:13:19.336458Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df.to_csv(\"./data/train.csv\",encoding='utf-8-sig',index = False)\n",
    "test_df.to_csv(\"./data/test.csv\",encoding='utf-8-sig',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9b6fc51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:20.492033Z",
     "start_time": "2022-07-13T02:13:20.488363Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = './data/train.csv'\n",
    "test_data = './data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dcc8bc21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:21.984400Z",
     "start_time": "2022-07-13T02:13:20.968915Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7f5f2aeeea528611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\jinyo\\.cache\\huggingface\\datasets\\csv\\default-7f5f2aeeea528611\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 500.22it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\jinyo\\.cache\\huggingface\\datasets\\csv\\default-7f5f2aeeea528611\\0.0.0\\51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 95.87it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, ReadInstruction\n",
    "\n",
    "dataset = load_dataset('csv', data_files={'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e3aba135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:22.899391Z",
     "start_time": "2022-07-13T02:13:22.880243Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Date', 'Title', '주가의 날짜', '등락률', 'updown', 'Pos', 'Neg', 'Mid', '감성지수'],\n",
       "        num_rows: 4050\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Date', 'Title', '주가의 날짜', '등락률', 'updown', 'Pos', 'Neg', 'Mid', '감성지수'],\n",
       "        num_rows: 450\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bb7f8dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:24.829179Z",
     "start_time": "2022-07-13T02:13:24.819062Z"
    }
   },
   "outputs": [],
   "source": [
    "# labels = [label for label in dataset['train'].features.keys() if label in ['Pos','Neg','Mid']]\n",
    "# id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "# label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "# print(labels)\n",
    "# print(id2label)\n",
    "# print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6c26f577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:13:25.521953Z",
     "start_time": "2022-07-13T02:13:25.506563Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# import numpy as np\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\") # KlueBERT의 tokenizer를 활용합니다.\n",
    "\n",
    "# def preprocess_data(examples):\n",
    "#     # 배치화된 텍스트를 받습니다.\n",
    "#     text = examples[\"Title\"]\n",
    "#     # 인코딩 합니다.\n",
    "#     encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=100)\n",
    "#     # 라벨을 배치로 만들어줍니다.\n",
    "#     labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "#     # numpy array로 만들기 위해 0 매트릭스를 만들어줍니다.\n",
    "#     labels_matrix = np.zeros((len(text), len(labels)))\n",
    "#     # 채웁니다.\n",
    "#     for idx, label in enumerate(labels):\n",
    "#         labels_matrix[:, idx] = labels_batch[label]\n",
    "#         encoding[\"labels\"] = labels_matrix.tolist()\n",
    "#         return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fea84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e74209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb987061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbd207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b8d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2577abd0",
   "metadata": {},
   "source": [
    "## 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b7f26924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:12:20.109370Z",
     "start_time": "2022-07-13T02:12:20.098794Z"
    }
   },
   "outputs": [],
   "source": [
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1038e901",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:09:20.401247Z",
     "start_time": "2022-07-13T03:09:04.136791Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 336/336 [00:00<00:00, 56.8kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 140k/140k [00:00<00:00, 245kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 287k/287k [00:00<00:00, 373kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 33.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\") # KlueBERT의 tokenizer를 활용합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\") # FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b4710ba5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:09:47.825099Z",
     "start_time": "2022-07-13T03:09:47.807623Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "\n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "\n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        # input_id는 워드 임베딩을 위한 문장의 정수 인코딩\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "\n",
    "        # attention_mask는 실제 단어가 위치하면 1, 패딩의 위치에는 0인 시퀀스.\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "\n",
    "        # token_type_id은 세그먼트 인코딩\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "262d0268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:09:48.940854Z",
     "start_time": "2022-07-13T03:09:48.182439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/4050 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\jinyo\\.conda\\envs\\py123\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 4050/4050 [00:00<00:00, 6509.98it/s]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y = convert_examples_to_features(dataset['train']['Title'], dataset['train']['감성지수'], max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "05cd5494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:09:49.589958Z",
     "start_time": "2022-07-13T03:09:49.502447Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/450 [00:00<?, ?it/s]C:\\Users\\jinyo\\.conda\\envs\\py123\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 450/450 [00:00<00:00, 7428.81it/s]\n"
     ]
    }
   ],
   "source": [
    "test_X, test_y = convert_examples_to_features(dataset['test']['Title'],  dataset['test']['감성지수'], max_seq_len=128, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c1dfabd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:09:50.380868Z",
     "start_time": "2022-07-13T03:09:50.351412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어에 대한 정수 인코딩 : [    2  3099  5719  8473 10120 10016  5070 12491  3389  6602  6066  5128\n",
      "  4119  5436 10330 11979     3     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "어텐션 마스크 : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "세그먼트 인코딩 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "각 인코딩의 길이 : 128\n",
      "정수 인코딩 복원 : [CLS] 비올 일본 최대 병원체인과 실펌엑스 총판계약 체결 [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "레이블 : 1\n"
     ]
    }
   ],
   "source": [
    "input_id = train_X[0][0]\n",
    "attention_mask = train_X[1][0]\n",
    "token_type_id = train_X[2][0]\n",
    "label = train_y[0]\n",
    "\n",
    "print('단어에 대한 정수 인코딩 :',input_id)\n",
    "print('어텐션 마스크 :',attention_mask)\n",
    "print('세그먼트 인코딩 :',token_type_id)\n",
    "print('각 인코딩의 길이 :', len(input_id))\n",
    "print('정수 인코딩 복원 :',tokenizer.decode(input_id))\n",
    "print('레이블 :',label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c696f21",
   "metadata": {},
   "source": [
    "## 모델-KlueBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad96713",
   "metadata": {},
   "source": [
    "### multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "41b3430c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:10:19.137960Z",
     "start_time": "2022-07-13T03:09:53.438799Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 655/655 [00:00<00:00, 164kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 387M/387M [00:19<00:00, 20.9MB/s]\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=3, from_pt=True)\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"snunlp/KR-FinBert\", num_labels=3, from_pt=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.categorical_hinge\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c20ec11d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T02:29:35.518343Z",
     "start_time": "2022-07-13T02:29:35.505747Z"
    }
   },
   "outputs": [],
   "source": [
    "# early_stopping = EarlyStopping(\n",
    "#     monitor=\"val_accuracy\", \n",
    "#     min_delta=0.001,\n",
    "#     patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "685e26d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T03:11:03.531418Z",
     "start_time": "2022-07-13T03:10:21.720164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "  1/102 [..............................] - ETA: 46:57 - loss: 29.3469 - accuracy: 0.1875"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [134]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;43;03m#     ,callbacks = [early_stopping]\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.conda\\envs\\py123\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_X, train_y, epochs=2, batch_size=32, validation_split=0.2\n",
    "#     ,callbacks = [early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e6e7a",
   "metadata": {},
   "source": [
    "## 성능평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3240a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912d802",
   "metadata": {},
   "source": [
    "snunlp/KR-FinBert-SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9031a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"./model/checkpoint-best-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81262b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"삼성SDI 적자 전환\"\n",
    "\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = trainer.model(**encoding)   # (**encoding) input으로 encoding된 숫자값이 들어가도록 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfec23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits # labels에 대해 리뷰를 기준으로 모델이 뱉은 각각의 숫자값 (실수)-> logits이라고 함\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply sigmoid + threshold   # 확률값 중에 뭐를 기준으로 해서 결과를 뱉어 낼거냐. threshold가 높을수록 아주 높은 값만을 라벨링하게 됨\n",
    "sigmoid = torch.nn.Sigmoid()   #  sigmoid 함수 호출\n",
    "probs = sigmoid(logits.squeeze().cpu())  # 각각에 대해 확률값으로 바꿔주는 것 \n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.1)] = 1  # threshold 값 : 0.1 --> probs이 0.1 이상인 값들만 1로 바꾸고, 나머지는 0으로 남김  -->  1로 해당하는 값이 최종 라벨링된 값\n",
    "                                                                            # threshold 값 정하는 기준 : 경험적 or ROC 커브(threshold를 계속 바꿔가면서 그래프에 점을 찍어서 curve를 그린것, FPR가 낮고-TPR가 높은 것에 가까운 점을 갖는(AUC를 가장 높게 만드는) threshold를 찾는것)\n",
    "\n",
    "predicted_labels = [id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f5cde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KR-FinBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f096cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"snunlp/KR-FinBert\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"snunlp/KR-FinBert\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
